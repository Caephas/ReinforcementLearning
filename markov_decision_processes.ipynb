{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5902fb",
   "metadata": {},
   "source": [
    "## Simulating an MDP with Value function and Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32078b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521646eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MDP components\n",
    "\n",
    "states = [0,1,2] # S\n",
    "actions= [0, 1] # A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ba43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition probabilites: T[s][a][s'] = P(s' | s, a)\n",
    "\n",
    "T = {\n",
    "    0: {\n",
    "        0: [0.8, 0.2, 0.0], # From state 0, action 0 -> mostly to 0 and sometimes to 1\n",
    "        1: [0.0, 1.0, 0.0] # From state 0, action 1 -> always to 1\n",
    "    },\n",
    "    1: {\n",
    "        0: [0.0, 0.0, 1.0], # From state 1, action 0 -> always to 2(terminal)\n",
    "        1: [0.5, 0.5, 0.0], # From state 1, action 1 -> go to 0 or 1\n",
    "    },\n",
    "    2: {\n",
    "        0: [0.0, 0.0, 1.0], # Terminal state: stays in 2\n",
    "        1: [0.0, 0.0, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Rewards: R[s][a] = expected reward for taking action a in state s\n",
    "R = {\n",
    "    0: {0: 5, 1:10},\n",
    "    1: {0: -10, 1: 0},\n",
    "    2: {0: 0, 1:0}\n",
    "}\n",
    "\n",
    "gamma = 0.9 # Discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9059be93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "V: [10.  0.  0.]\n",
      "Q:\n",
      " [[  5.  10.]\n",
      " [-10.   0.]\n",
      " [  0.   0.]]\n",
      "----------------------------------------\n",
      "Iteration 2\n",
      "V: [12.2  4.5  0. ]\n",
      "Q:\n",
      " [[ 12.2  10. ]\n",
      " [-10.    4.5]\n",
      " [  0.    0. ]]\n",
      "----------------------------------------\n",
      "Iteration 3\n",
      "V: [14.594  7.515  0.   ]\n",
      "Q:\n",
      " [[ 14.594  14.05 ]\n",
      " [-10.      7.515]\n",
      " [  0.      0.   ]]\n",
      "----------------------------------------\n",
      "Iteration 4\n",
      "V: [16.86038  9.94905  0.     ]\n",
      "Q:\n",
      " [[ 16.86038  16.7635 ]\n",
      " [-10.        9.94905]\n",
      " [  0.        0.     ]]\n",
      "----------------------------------------\n",
      "Iteration 5\n",
      "V: [18.954145  12.0642435  0.       ]\n",
      "Q:\n",
      " [[ 18.9303026  18.954145 ]\n",
      " [-10.         12.0642435]\n",
      " [  0.          0.       ]]\n",
      "----------------------------------------\n",
      "Iteration 6\n",
      "V: [20.85781915 13.95827483  0.        ]\n",
      "Q:\n",
      " [[ 20.81854823  20.85781915]\n",
      " [-10.          13.95827483]\n",
      " [  0.           0.        ]]\n",
      "----------------------------------------\n",
      "Iteration 7\n",
      "V: [22.56244734 15.66724229  0.        ]\n",
      "Q:\n",
      " [[ 22.53011926  22.56244734]\n",
      " [-10.          15.66724229]\n",
      " [  0.           0.        ]]\n",
      "----------------------------------------\n",
      "Iteration 8\n",
      "V: [24.10051806 17.20336033  0.        ]\n",
      "Q:\n",
      " [[ 24.0650657   24.10051806]\n",
      " [-10.          17.20336033]\n",
      " [  0.           0.        ]]\n",
      "----------------------------------------\n",
      "Iteration 9\n",
      "V: [25.4830243  18.58674528  0.        ]\n",
      "Q:\n",
      " [[ 25.44897786  25.4830243 ]\n",
      " [-10.          18.58674528]\n",
      " [  0.           0.        ]]\n",
      "----------------------------------------\n",
      "Iteration 10\n",
      "V: [26.72807075 19.83139631  0.        ]\n",
      "Q:\n",
      " [[ 26.69339165  26.72807075]\n",
      " [-10.          19.83139631]\n",
      " [  0.           0.        ]]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Intialize value and Q functions\n",
    "\n",
    "V = np.zeros(len(states))\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "\n",
    "# Value Iteration (for V and Q)\n",
    "for iteration in range(10): \n",
    "    V_new = np.zeros_like(V)\n",
    "    Q_new = np.zeros_like(Q)\n",
    "\n",
    "    for s in states:\n",
    "        for a in actions:\n",
    "            expected_value = 0\n",
    "            for s_prime in states:\n",
    "                expected_value += T[s][a][s_prime] * (R[s][a] + gamma * V[s_prime])\n",
    "            Q_new[s, a] = expected_value\n",
    "\n",
    "        # Bellman's optimality: pick best Q for this state\n",
    "        V_new[s] = np.max(Q_new[s])\n",
    "    \n",
    "    V = V_new\n",
    "    Q = Q_new\n",
    "\n",
    "    print(f'Iteration {iteration + 1}')\n",
    "    print('V:', V)\n",
    "    print('Q:\\n', Q)\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5409f4fe",
   "metadata": {},
   "source": [
    "simulating a Markov Decision Process (MDP) with:\n",
    "\n",
    "\t•\t3 states: s0, s1, s2\n",
    "\t•\t2 actions per state (e.g., a0, a1)\n",
    "\t\n",
    "\tDefined:\n",
    "\t\n",
    "\t•\tTransition probabilities T(s, a, s')\n",
    "\t•\tRewards R(s, a, s')\n",
    "\t•\tDiscount factor γ = 0.9\n",
    "\n",
    "Value Iteration Recap\n",
    "\n",
    "The algorithm iteratively updates:\n",
    "\n",
    "\t•\tV(s) → the value of being in state s\n",
    "\t•\tQ(s, a) → the value of taking action a in state s\n",
    "\n",
    "Using the update rules:\n",
    "\n",
    "What my Output Shows\n",
    "\n",
    "\t•\tV(s) values are increasing over iterations → converging toward optimal state values.\n",
    "\t•\tQ(s, a) shows the value of specific actions in each state.\n",
    "\t\n",
    "\tAfter ~10 iterations:\n",
    "\n",
    "\t•\tV[0] ≈ 26.7 → starting from s0, optimal expected return.\n",
    "\t•\tQ[0]: You can compare Q[0,0] vs Q[0,1] to choose best action in s0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a446e30",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
